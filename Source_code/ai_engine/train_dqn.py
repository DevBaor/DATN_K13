import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import random

from db import load_posts, load_all_utilities  # TODO: n·∫øu c√≥ h√†m load_favorites th√¨ import th√™m
from dqn_model import DQN
from utils import RoomEnv

# ======================================================
# 1. LOAD DATA T·ª™ DATABASE
# ======================================================
df = load_posts()
posts = df.to_dict(orient="records")

# utilities: {phong_id: [id_dich_vu1, id_dich_vu2, ...]}
utilities = load_all_utilities()

# favorites: t·∫°m th·ªùi ƒë·ªÉ tr·ªëng (n·∫øu m ƒë√£ c√≥ b·∫£ng yeu_thich
# th√¨ c√≥ th·ªÉ t·ª± build list phong_id v√†o ƒë√¢y)
favorites = []  # v√≠ d·ª•: [1, 5, 9]

# Chu·∫©n h√≥a rooms cho RoomEnv
rooms = []
for p in posts:
    # b·∫Øt bu·ªôc ph·∫£i c√≥ c√°c field n√†y trong load_posts():
    #   id, gia_thue (ho·∫∑c gia), dien_tich, days_empty
    room = {
        "id": p["id"],
        "gia_thue": float(p.get("gia_thue", p.get("gia", 0))),  # fallback n·∫øu c·ªôt t√™n 'gia'
        "dien_tich": float(p.get("dien_tich", 0)),
        # n·∫øu db ch∆∞a c√≥ days_empty th√¨ m t√≠nh ·ªü db.py d·ª±a tr√™n phong + hop_dong
        "days_empty": float(p.get("days_empty", 0)),
    }
    rooms.append(room)

env = RoomEnv(rooms, utilities, favorites)

# ======================================================
# 2. C·∫§U H√åNH USER "GI·∫¢" KHI TRAIN
#    (l√∫c deploy th·ª±c t·∫ø s·∫Ω l·∫•y t·ª´ UI)
# ======================================================
user = {
    "max_price": 3_000_000,   # ng√¢n s√°ch (VNƒê)
    "area": 20,               # di·ªán t√≠ch mong mu·ªën (m2)
    "utilities": [1, 2, 3],   # id d·ªãch v·ª• mong mu·ªën (v√≠ d·ª•)
}

# L·∫•y state ban ƒë·∫ßu v√† flatten th√†nh vector 1D
state = env.reset(user)
state = state.reshape(-1)
state_size = state.shape[0]
action_size = env.num_actions

print("STATE SIZE:", state_size)
print("ACTIONS (s·ªë ph√≤ng):", action_size)

# ======================================================
# 3. KH·ªûI T·∫†O M√î H√åNH DQN
# ======================================================
policy_net = DQN(state_size, action_size)
target_net = DQN(state_size, action_size)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=0.001)

memory = []
BATCH = 32
GAMMA = 0.99
MAX_MEMORY = 5000

# Epsilon-greedy cho exploration
EPS_START = 1.0
EPS_END = 0.05
EPS_DECAY = 300
steps_done = 0


# ======================================================
# 4. H√ÄM CH·ªåN ACTION (PH√íNG)
# ======================================================
def select_action(state_vec):
    global steps_done

    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        np.exp(-1.0 * steps_done / EPS_DECAY)
    steps_done += 1

    if random.random() < eps_threshold:
        # random 1 ph√≤ng
        return random.randrange(action_size)
    else:
        with torch.no_grad():
            s = torch.tensor(state_vec, dtype=torch.float32).unsqueeze(0)
            q_values = policy_net(s)
            return int(torch.argmax(q_values).item())


# ======================================================
# 5. H√ÄM TRAIN 1 BATCH T·ª™ REPLAY MEMORY
# ======================================================
def train_step():
    if len(memory) < BATCH:
        return

    batch = random.sample(memory, BATCH)
    states, actions, rewards, next_states = zip(*batch)

    states = torch.tensor(states, dtype=torch.float32)
    next_states = torch.tensor(next_states, dtype=torch.float32)
    rewards = torch.tensor(rewards, dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.long)

    # Q(s, a)
    q_values = policy_net(states)
    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)

    # max_a' Q_target(s', a')
    next_q = target_net(next_states).max(1)[0].detach()
    expected = rewards + GAMMA * next_q

    loss = nn.MSELoss()(q_value, expected)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()


# ======================================================
# 6. V√íNG L·∫∂P TRAIN
# ======================================================
EPISODES = 400

for ep in range(EPISODES):
    # reset m√¥i tr∆∞·ªùng v·ªõi c√πng 1 user gi·∫£
    state = env.reset(user)
    state = state.reshape(-1)

    # ch·ªçn action theo ch√≠nh s√°ch epsilon-greedy
    action = select_action(state)

    # m√¥i tr∆∞·ªùng tr·∫£ v·ªÅ reward cho action ƒë√≥
    next_state, reward, done, _ = env.step(action)
    next_state = next_state.reshape(-1)

    # l∆∞u v√†o replay memory
    if len(memory) >= MAX_MEMORY:
        memory.pop(0)
    memory.append((state, action, reward, next_state))

    # train 1 step
    train_step()

    # update target_net ƒë·ªãnh k·ª≥
    if ep % 40 == 0:
        target_net.load_state_dict(policy_net.state_dict())
        print(f"Episode: {ep}, reward: {reward:.3f}")

# ======================================================
# 7. L∆ØU MODEL
# ======================================================
torch.save(policy_net.state_dict(), "dqn_room.pt")
print("üî• Train xong model dqn_room.pt v·ªõi state_size =", state_size)
